<!DOCTYPE html>
<html lang="zh-cn">
<head>
<meta charset="utf-8">
<meta name="description" content="">
<meta name="generator" content="Hugo 0.62.0" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="https://liushaoxiong10.github.io/css/style.css" type="text/css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700" type="text/css">
<link rel="alternate" href="https://liushaoxiong10.github.io/index.xml" type="application/rss+xml" title="K0ala 云原生">
<title>kubernetes 基本概念 - K0ala 云原生</title>
</head>
<body>

<header>
  <div class="container clearfix">
    <a class="path" href="https://liushaoxiong10.github.io/">[K0ala 云原生]</a>
    <span class="caret"># _</span>
    <div class="right">
      
    </div>
  </div>
</header>

<div class="container">


<main role="main" class="article">
  
<article class="single" itemscope itemtype="http://schema.org/BlogPosting">
  <div class="meta">
    
    <span class="key">published on</span>
    <span class="val"><time itemprop="datePublished"
        datetime="2020-01-18">January 18, 2020</time></span>

    

    
    <span class="key">in</span>
    <span class="val">
      
      <a href="https://liushaoxiong10.github.io/categories/kubernetes%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E7%AC%94%E8%AE%B0">kubernetes权威指南(笔记)</a>
      
    </span>
    
    
    <br>
    <span class="key">tags:</span>
    <span class="val">
      
      <a href="https://liushaoxiong10.github.io/tags/k8s">k8s</a>
      
    </span>
    
  </div>

  <h1 class="headline" itemprop="headline">kubernetes 基本概念</h1>
  <section class="body" itemprop="articleBody">
    <h1 id="kubernetes">Kubernetes是什么？</h1>
<p>Kubernetes是一个完备的分布式系统支撑平台。Kubernetes具有完备的集群管理能力，包括多层次的安全防护和准入机制、多租户应用支撑能力、透明的服务注册和服务发现机制、内建的智能负载均衡器、强大的故障发现和自我修复能力、服务滚动升级和在线扩容能力、可扩展的资源自动调度机制，以及多粒度的资源配额管理能力。</p>
<p>同时，Kubernetes提供了完善的管理工具，这些工具涵盖了包括开发、部署测试、运维监控在内的各个环节。因此，Kubernetes是一个全新的基于容器技术的分布式架构解决方案，并且是一个一站式的完备的分布式系统开发和支撑平台。</p>
<h1 id="heading">基本概念</h1>
<h2 id="master">Master</h2>
<p>Master 指的是集群的控制节点，每个 kubernetes 都需要一个 master 来负责整个集群的管理和控制。</p>
<p>Master 上运行以下组件：</p>
<ul>
<li>kubernetes API Server（kube-apiserver）：提供了HTTP Rest接口的关键服务进程，是Kubernetes里所有资源的增、删、改、查等操作的唯一入口，也是集群控制的入口进程。</li>
<li>kubernetes Controller Manager（kube-controller-manager）: kubernetes的所有资源对象的自动化控制中心。</li>
<li>kubernetes Scheduler（kube-scheduler）: 负责资源调度（pod调度）的进程。</li>
</ul>
<h2 id="node">Node</h2>
<p>Node是Kubernetes集群中的工作负载节点，每个Node都会被Master分配一些工作负载（Docker容器），当某个Node宕机时，其上的工作负载会被Master自动转移到其他节点上。</p>
<p>每个节点上都应有以下组件：</p>
<ul>
<li>kubelet : 负责Pod的创建、启停等任务，与 Master 协作，实现集群管理。</li>
<li>kube-proxu : 实现Kubernetes Service的通信与负载均衡机制的重要组件。</li>
<li>Docker Engine（docker）：Docker引擎，负责本机的容器创建和管理工作。</li>
</ul>
<h2 id="pod">Pod</h2>
<p>Pod是Kubernetes最重要的基本概念，下图为一个pod示意图，每个Pod都有一个特殊的被称为“根容器”的Pause容器，Pause容器对应的镜像属于Kubernetes平台的一部分，除了Pause容器，每个Pod还包含一个或多个紧密相关的用户业务容器。</p>
<p><img src="https://liushaoxiong10.github.io/img/kubernetes/pod.png" alt="pod"></p>
<p>Pod有两种类型：</p>
<ul>
<li>
<p>普通pod</p>
<p>普通的pod被创建，就会被存储在 etcd 中，随后会被master调度到某个 node 并进行绑定，之后 node 上的 kubelet 会将该 pod 转换为一组相关的 docker 容器并启动。</p>
</li>
<li>
<p>静态pod</p>
<p>没有被存储在 etcd 中，而是在某个 node 上的文件中，并且只在该 node 启动运行。</p>
</li>
</ul>
<p>pod， node 和 容器的关系如下：
<img src="https://liushaoxiong10.github.io/img/kubernetes/node.png" alt="node"></p>
<h2 id="label">Label</h2>
<p>Label（标签）是Kubernetes系统中另外一个核心概念。一个Label是一个key=value的键值对，其中key与value由用户自己指定。Label可以被附加到各种资源对象上，例如Node、Pod、Service、RC等，一个资源对象可以定义任意数量的Label，同一个Label也可以被添加到任意数量的资源对象上。Label通常在资源对象定义时确定，也可以在对象创建后动态添加或者删除。</p>
<p>Label相当于我们熟悉的“标签”。给某个资源对象定义一个Label，就相当于给它打了一个标签，随后可以通过Label Selector（标签选择器）查询和筛选拥有某些Label的资源对象，Kubernetes通过这种方式实现了类似SQL的简单又通用的对象查询机制。</p>
<p>Label Selector 匹配方法：</p>
<ul>
<li>name=k0ala : 匹配所有<strong>具有</strong> name=k0ala 标签的资源对象。</li>
<li>name!=k0ala : 匹配所有<strong>不具有</strong> name=k0ala 标签的资源对象。</li>
<li>name in (k0ala, justin) : 匹配所有<strong>具有</strong> name=k0ala 或者 name=justin 标签的资源对象。</li>
<li>name not in (k0ala) : 匹配所有<strong>不具有</strong> name=k0ala 标签的资源对象。</li>
</ul>
<h2 id="replication-controller">Replication Controller</h2>
<p>RC是Kubernetes系统中的核心概念之一，简单来说，它其实定义了一个期望的场景，即声明某种Pod的副本数量在任意时刻都符合某个预期值，所以RC的定义包括如下几个部分。</p>
<ul>
<li>Pod期待的副本数量。</li>
<li>用于筛选目标Pod的Label Selector。</li>
<li>当Pod的副本数量小于预期数量时，用于创建新Pod的Pod模板（template）。</li>
</ul>
<p>在Kubernetes 1.2中，升级为另外一个新概念—Replica Set，官方解释其为“下一代的RC”。Replica Set与RC当前的唯一区别是，Replica Sets支持基于集合的Label selector（Set-based selector），而RC只支持基于等式的Label Selector（equality-based selector），这使得Replica Set的功能更强。</p>
<p>但是我们当前很少单独使用Replica Set，它主要被Deployment这个更高层的资源对象所使用，从而形成一整套Pod创建、删除、更新的编排机制。</p>
<p>RC（Replica Set）的一些特性与作用。</p>
<ul>
<li>在大多数情况下，我们通过定义一个RC实现Pod的创建及副本数量的自动控制。</li>
<li>在RC里包括完整的Pod定义模板。</li>
<li>RC通过Label Selector机制实现对Pod副本的自动控制。</li>
<li>通过改变RC里的Pod副本数量，可以实现Pod的扩容或缩容。</li>
<li>通过改变RC里Pod模板中的镜像版本，可以实现Pod的滚动升级。</li>
</ul>
<h2 id="deployment">Deployment</h2>
<p>Deployment是Kubernetes在1.2版本中引入的新概念，用于更好地解决Pod的编排问题。为此，Deployment在内部使用了Replica Set来实现目的，无论从Deployment的作用与目的、YAML定义，还是从它的具体命令行操作来看，我们都可以把它看作RC的一次升级，两者的相似度超过90%。</p>
<p>Deployment的典型使用场景有以下几个。</p>
<ul>
<li>创建一个Deployment对象来生成对应的Replica Set并完成Pod副本的创建。</li>
<li>检查Deployment的状态来看部署动作是否完成（Pod副本数量是否达到预期的值）。</li>
<li>更新Deployment以创建新的Pod（比如镜像升级）。</li>
<li>如果当前Deployment不稳定，则回滚到一个早先的Deployment版本。</li>
<li>暂停Deployment以便于一次性修改多个PodTemplateSpec的配置项，之后再恢复Deployment，进行新的发布。</li>
<li>扩展Deployment以应对高负载。</li>
<li>查看Deployment的状态，以此作为发布是否成功的指标。</li>
<li>清理不再需要的旧版本ReplicaSets。</li>
</ul>
<h2 id="horizontal-pod-autoscaler">Horizontal Pod Autoscaler</h2>
<p>Horizontal Pod Autoscaling（Pod横向自动扩容，HPA），与之前的RC、Deployment一样，也属于一种Kubernetes资源对象。通过追踪分析指定RC控制的所有目标Pod的负载变化情况，来确定是否需要有针对性地调整目标Pod的副本数量，这是HPA的实现原理。</p>
<p>HPA有以下两种方式作为Pod负载的度量指标：</p>
<ul>
<li>
<p>CPUUtilizationPercentage</p>
<p>一个算术平均值，即目标Pod所有副本自身的CPU利用率的平均值。</p>
</li>
<li>
<p>应用程序自定义的度量指标，比如服务在每秒内的相应请求数（TPS或QPS）</p>
</li>
</ul>
<h2 id="statefulset">StatefulSet</h2>
<p>在Kubernetes系统中，Pod的管理对象RC、Deployment、DaemonSet和Job都面向<strong>无状态的服务</strong>。但现实中有很多服务是<strong>有状态的</strong>，特别是一些复杂的中间件集群，例如MySQL集群、MongoDB集群、Akka集群、ZooKeeper集群等，这些应用集群有4个共同点。</p>
<ol>
<li>每个节点都有固定的身份ID，通过这个ID，集群中的成员可以相互发现并通信。</li>
<li>集群的规模是比较固定的，集群规模不能随意变动。</li>
<li>集群中的每个节点都是有状态的，通常会持久化数据到永久存储中。</li>
<li>如果磁盘损坏，则集群里的某个节点无法正常运行，集群功能受损。</li>
</ol>
<p>StatefulSet从本质上来说，可以看作Deployment/RC的一个特殊变种，它有如下特性：</p>
<ul>
<li>StatefulSet里的每个Pod都有稳定、唯一的网络标识，可以用来发现集群内的其他成员。</li>
<li>StatefulSet控制的Pod副本的启停顺序是受控的，操作第n个Pod时，前n-1个Pod已经是运行且准备好的状态。</li>
<li>StatefulSet里的Pod采用稳定的持久化存储卷，通过PV或PVC来实现，删除Pod时默认不会删除与StatefulSet相关的存储卷（为了保证数据的安全）。</li>
</ul>
<h2 id="service">Service</h2>
<p>Service服务也是Kubernetes里的核心资源对象之一，Kubernetes里的每个Service其实就是我们经常提起的微服务架构中的一个微服务，之前讲解Pod、RC等资源对象其实都是为讲解Kubernetes Service做铺垫的。</p>
<p>Service 是分布式集群架构的核心，它应具有以下特征：</p>
<ul>
<li>拥有唯一指定的名称</li>
<li>拥有一个虚拟IP</li>
<li>能够提供某种远程服务</li>
<li>被映射到提供这种服务能力的一组容器应用上</li>
</ul>
<h3 id="heading-1">服务架构</h3>
<p><img src="https://liushaoxiong10.github.io/img/kubernetes/service.png" alt="service"></p>
<p>Kubernetes的Service定义了一个服务的访问入口地址，前端的应用（Pod）通过这个入口地址访问其背后的一组由Pod副本组成的集群实例，Service与其后端Pod副本集群之间则是通过Label Selector来实现无缝对接的。RC的作用实际上是保证Service的服务能力和服务质量始终符合预期标准。</p>
<p>在负载均衡上，Service 没有共用一个负载均衡器的IP地址，每个Service都被分配了一个全局唯一的虚拟IP地址，这个虚拟IP被称为Cluster IP。这样一来，每个服务就变成了具备唯一IP地址的通信节点，服务调用就变成了最基础的TCP网络通信问题。</p>
<h3 id="heading-2">外部系统访问</h3>
<p>三种IP：</p>
<ul>
<li>Node IP ： Node IP是Kubernetes集群中每个节点的物理网卡的IP地址，是一个真实存在的物理网络</li>
<li>Pod IP ： Pod IP是每个Pod的IP地址，它是Docker Engine根据docker0网桥的IP地址段进行分配的，通常是一个虚拟的二层网络</li>
<li>Cluster IP ： Service的Cluster IP，它也是一种虚拟的IP，但更像一个“伪造”的IP网络</li>
</ul>
<p>之所以说 cluster ip 是”<strong>伪造</strong>“的因为以下几点：</p>
<ul>
<li>Cluster IP仅仅作用于Kubernetes Service这个对象，并由Kubernetes管理和分配IP地址（来源于Cluster IP地址池）。</li>
<li>Cluster IP无法被Ping，因为没有一个“实体网络对象”来响应。</li>
<li>Cluster IP只能结合Service Port组成一个具体的通信端口，单独的Cluster IP不具备TCP/IP通信的基础，并且它们属于Kubernetes集群这样一个封闭的空间，集群外的节点如果要访问这个通信端口，则需要做一些额外的工作。</li>
<li>在Kubernetes集群内，Node IP网、Pod IP网与Cluster IP网之间的通信，采用的是Kubernetes自己设计的一种编程方式的特殊路由规则，与我们熟知的IP路由有很大的不同。</li>
</ul>
<p>总结上面的原因：Service的Cluster IP属于Kubernetes集群内部的地址，无法在集群外部直接使用这个地址。所以就有了 <strong>Node Port</strong>。</p>
<p>NodePort的实现方式是在Kubernetes集群里的每个Node上都为需要外部访问的Service开启一个对应的TCP监听端口，外部系统只要用任意一个Node的IP地址+具体的NodePort端口号即可访问此服务。</p>
<h2 id="job">Job</h2>
<p>批处理任务通常并行（或者串行）启动多个计算进程去处理一批工作项（work item），在处理完成后，整个批处理任务结束。</p>
<p>与RC、Deployment、ReplicaSet、DaemonSet类似，Job也控制一组Pod容器。</p>
<p>Job也是一种特殊的Pod副本自动控制器，同时Job控制Pod副本与RC等控制器的工作机制有以下重要差别：</p>
<ul>
<li>Job所控制的Pod副本是短暂运行的，可以将其视为一组Docker容器，其中的每个Docker容器都仅仅运行一次。当Job控制的所有Pod副本都运行结束时，对应的Job也就结束了。Job在实现方式上与RC等副本控制器不同，Job生成的Pod副本是不能自动重启的，对应Pod副本的RestartPoliy都被设置为Never。</li>
<li>Job所控制的Pod副本的工作模式能够多实例并行计算，以TensorFlow框架为例，可以将一个机器学习的计算任务分布到10台机器上，在每台机器上都运行一个worker执行计算任务，这很适合通过Job生成10个Pod副本同时启动运算。</li>
</ul>
<h2 id="volume">Volume</h2>
<p>Volume（存储卷）是Pod中能够被多个容器访问的共享目录。具有以下特点：</p>
<ul>
<li>Kubernetes中的Volume被定义在Pod上，然后被一个Pod里的多个容器挂载到具体的文件目录下；</li>
<li>Kubernetes中的Volume与Pod的生命周期相同，但与容器的生命周期不相关，当容器终止或者重启时，Volume中的数据也不会丢失。</li>
<li>Kubernetes支持多种类型的Volume，例如GlusterFS、Ceph等先进的分布式文件系统。</li>
</ul>
<h3 id="volume-1">支持的Volume类型</h3>
<ol>
<li>
<p>emptyDir</p>
<p>在Pod分配到Node时创建的，Pod 销毁时删除。具有以下特点：</p>
<ul>
<li>临时空间，例如用于某些应用程序运行时所需的临时目录，且无须永久保留。</li>
<li>长时间任务的中间过程CheckPoint的临时保存目录。</li>
<li>一个容器需要从另一个容器中获取数据的目录（多容器共享目录）</li>
</ul>
</li>
<li>
<p>hostPath</p>
<p>hostPath为在Pod上挂载宿主机上的文件或目录，它通常可以用于以下几方面:</p>
<ul>
<li>容器应用程序生成的日志文件需要永久保存时，可以使用宿主机的高速文件系统进行存储。</li>
<li>需要访问宿主机上Docker引擎内部数据结构的容器应用时，可以通过定义hostPath为宿主机/var/lib/docker目录，使容器内部应用可以直接访问Docker的文件系统。</li>
</ul>
<p>使用时需要注意：</p>
<ul>
<li>在不同的Node上具有相同配置的Pod，可能会因为宿主机上的目录和文件不同而导致对Volume上目录和文件的访问结果不一致。</li>
<li>如果使用了资源配额管理，则Kubernetes无法将hostPath在宿主机上使用的资源纳入管理。</li>
</ul>
</li>
<li>
<p>gcePersistentDisk
使用这种类型的Volume表示使用谷歌公有云提供的永久磁盘（Persistent Disk，PD）存放Volume的数据，它与emptyDir不同，PD上的内容会被永久保存，当Pod被删除时，PD只是被卸载（Unmount），但不会被删除。</p>
</li>
<li>
<p>awsElasticBlockStore
该类型的Volume使用亚马逊公有云提供的EBS Volume存储数据，需要先创建一个EBS Volume才能使用awsElasticBlockStore。</p>
</li>
<li>
<p>NFS
使用NFS网络文件系统提供的共享目录存储数据时，我们需要在系统中部署一个NFS Server。</p>
</li>
<li>
<p>其他类型的Volume</p>
<ul>
<li>iscsi：使用iSCSI存储设备上的目录挂载到Pod中。</li>
<li>flocker：使用Flocker管理存储卷。</li>
<li>glusterfs：使用开源GlusterFS网络文件系统的目录挂载到Pod中。</li>
<li>rbd：使用Ceph块设备共享存储（Rados Block Device）挂载到Pod中。</li>
<li>gitRepo：通过挂载一个空目录，并从Git库clone一个git repository以供Pod使用。</li>
<li>secret：一个Secret Volume用于为Pod提供加密的信息，你可以将定义在Kubernetes中的Secret直接挂载为文件让Pod访问。Secret Volume是通过TMFS（内存文件系统）实现的，这种类型的Volume总是不会被持久化的。</li>
</ul>
</li>
</ol>
<h2 id="persistent-volume">Persistent Volume</h2>
<p>之前提到的Volume是被定义在Pod上的，属于计算资源的一部分，而实际上，网络存储是相对独立于计算资源而存在的一种实体资源。</p>
<p>PV可以被理解成Kubernetes集群中的某个网络存储对应的一块存储，它与Volume类似，但有以下区别。</p>
<ul>
<li>PV只能是网络存储，不属于任何Node，但可以在每个Node上访问。</li>
<li>PV并不是被定义在Pod上的，而是独立于Pod之外定义的。</li>
<li>PV目前支持的类型包括：gcePersistentDisk、AWSElasticBlockStore、AzureFile、AzureDisk、FC（Fibre Channel）、Flocker、NFS、iSCSI、RBD（Rados Block Device）、CephFS、Cinder、GlusterFS、VsphereVolume、Quobyte Volumes、VMware Photon、Portworx Volumes、ScaleIO Volumes和HostPath（仅供单机测试）。</li>
</ul>
<p>比较重要的是PV的accessModes属性，目前有以下类型。</p>
<ul>
<li>ReadWriteOnce：读写权限，并且只能被单个Node挂载。</li>
<li>ReadOnlyMany：只读权限，允许被多个Node挂载。</li>
<li>ReadWriteMany：读写权限，允许被多个Node挂载。</li>
</ul>
<p>PV是有状态的对象，它的状态有以下几种。</p>
<ul>
<li>Available：空闲状态。</li>
<li>Bound：已经绑定到某个PVC上。</li>
<li>Released：对应的PVC已经被删除，但资源还没有被集群收回。</li>
<li>Failed：PV自动回收失败。</li>
</ul>
<h2 id="namespace">Namespace</h2>
<p>Namespace（命名空间）是Kubernetes系统中的另一个非常重要的概念，Namespace在很多情况下用于实现多租户的资源隔离。</p>
<h2 id="annotation">Annotation</h2>
<p>Annotation（注解）与Label类似，也使用key/value键值对的形式进行定义。不同的是Label具有严格的命名规则，它定义的是Kubernetes对象的元数据（Metadata），并且用于Label Selector。Annotation则是用户任意定义的附加信息，以便于外部工具查找。</p>
<p>用Annotation来记录的信息如下：</p>
<ul>
<li>build信息、release信息、Docker镜像信息等，例如时间戳、release id号、PR号、镜像Hash值、Docker Registry地址等。</li>
<li>日志库、监控库、分析库等资源库的地址信息。</li>
<li>程序调试工具信息，例如工具名称、版本号等。</li>
<li>团队的联系信息，例如电话号码、负责人名称、网址等。</li>
</ul>
<h2 id="configmap">ConfigMap</h2>
<p>Kubernetes提供了一种内建机制，将存储在etcd中的ConfigMap通过Volume映射的方式变成目标Pod内的配置文件，不管目标Pod被调度到哪台服务器上，都会完成自动映射。进一步地，如果ConfigMap中的key-value数据被修改，则映射到Pod中的“配置文件”也会随之自动更新。于是，Kubernetes ConfigMap就成了分布式系统中最为简单（使用方法简单，但背后实现比较复杂）且对应用无侵入的配置中心。</p>
  </section>

</article>
  <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "disqus_FQaJ22nPXz" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</main>

</div>

<footer>
  <div class="container">
    <span class="copyright">&copy; 2020 K0ala 云原生 - <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></span>
  </div>
</footer>

</body>
</html>
